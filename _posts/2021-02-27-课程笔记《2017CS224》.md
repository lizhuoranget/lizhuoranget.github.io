---
layout:     post
title:      课程笔记《2017CS224》
subtitle:   
date:       2020-02-27
author:     Zhuoran Li
header-img: 
catalog:    true
tags:
- 笔记
---

## 第一讲

NLP的定义

---

## 第二讲

one-hot编码：以一个符号存储一个单词，任意两个单词的点积为0

分布相似性：通过对单词出现的大量上下文例句处理，得到单词的表示

"You shall know a word by the company it keeps"	--J.R.Firth 1957:11

现代统计学NLP：你可以预测单词的上下文，就可以理解单词的含义。

给每个单词够造一个向量，使得可以预测上下文单词。

​	$$ p(context|w_t)  $$

损失函数：

J = 1-p(w_-t|w_t)

-t是围绕在t周围的其他词

**低维词汇向量**

word2vec软件

两个算法：

* **skip-gram**：中心词预测周围词

* CBOW

两个训练方法：

* Hierarchical softmax
* Negative sampling

课程讲skip-gram和一个低效率包含基本概念的训练方法--naive softmax。

![1614582525833](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614582525833.png)

每一个估算步都取一个词作为中心词，这里是banking，预测其周围的词

对每一个词只有一个概率分布

![1614582752686](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614582752686.png)

$ \theta $就是词汇的向量表示

![1614583105688](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614583105688.png)

![1614583395956](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614583395956.png)

![1614583441194](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614583441194.png)

每个单词都有两个向量表示，即作为中心词向量v和上下文向量u。

![1614587297518](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614587297518.png)

![1614587341283](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614587341283.png)

![1614588715214](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614588715214.png)

![1614588921624](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614588921624.png)



---

## 第三讲

word2vec

![1614823020821](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614823020821.png)

![1614823300673](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614823300673.png)

第二项是随机抽取5或10个不相邻的单词，最小化和中心词相邻的概率

T是第t个时间步，或第t个窗口

可以遍历1个或5个窗口更新一次，但不想遍历完所有窗口做一次大更新，效果通常不好。

![1614825107505](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614825107505.png)

![1614825307743](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614825307743.png)

![1614825483088](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614825483088.png) 

![1614826181619](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614826181619.png)

下游任务如果有新单词不好处理。

如果词数量很大，需要降维。

![1614826396904](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614826396904.png)

![1614826845211](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614826845211.png)

长尾效应表示：不经常出现的词尝尝含有非常多的语义内容。

![1614827199087](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614827199087.png)

![1614827342841](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614827342841.png)

组合二者的优点，结果就是Glove模型

![1614827397424](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614827397424.png)

$P_{ij}$是共现矩阵，希望最小化内积距离和两个词计数的对数。

![1614827809640](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614827809640.png)

u和v是列向量和行向量，本质上是可以互换的，所以加起来更有意义。

![1614828836859](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614828836859.png)

polysemy一词多义

![1614829204451](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614829204451.png)

![1614833775206](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614833775206.png)

词向量的内在评估

![1614834393344](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614834393344.png)

![1614837963301](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614837963301.png)

![1614837979956](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614837979956.png)

![1614838184270](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614838184270.png)

![1614839050761](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614839050761.png)

y是行数，W_y就是类别得一个行向量

![1614839089294](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614839089294.png)

---

## 第四讲

![1614839420735](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614839420735.png)

分类

![1614840329176](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614840329176.png)

![1614843525057](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614843525057.png)

传统的分类问题，固定x，学习参数确定一个决策边界

![1614844909031](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614844909031.png)

W的第y行乘x得到一个数

对C行都乘x，结果归一化后和为1。

![1614845159294](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614845159294.png)

![1614845214095](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614845214095.png)

理想的概率记为p，softmax计算出来的概率记为q。我们的例子中，p是独热向量，所以非正确类别的求和项的值全部为零。

交叉熵可以理解为尽可能地最小化两个分布之间地KL发散。

p(c)是类别c得概率，是一个数值，不是向量。

![1614846234595](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614846234595.png)

![1614846759141](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614846759141.png)

正则使模型中得权重都尽可能小，防止过拟合。

x轴可以看作是不同得东西，比如模型多强大、多少参数、多少层数、词向量多少维度、训练时长，可以看到x轴对不同变量有相同的模式。y轴是误差或目标函数，尽可能地最小化。蓝色是训练误差，红色是验证误差。

![1614849622100](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614849622100.png)

![1614849671103](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614849671103.png)

![1614849767096](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614849767096.png)

![1614849844233](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614849844233.png)

![1614849913064](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614849913064.png)

![1614849983674](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614849983674.png)

![1614850085096](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850085096.png)

![1614850121336](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850121336.png)

给中心词分配一个标签，然后用一个窗口把它前后的单词连接起来

![1614850288347](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850288347.png)

![1614850477327](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850477327.png)

![1614850705457](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850705457.png)

![1614850859232](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850859232.png)

![1614850958341](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614850958341.png)

关于f的所有元素的所有导数，有这么一个规律，对正确类别-1，对其他类别什么也不做。

tip5就是对tip4的公式向量化，方便计算，t是独热目标概率分布。

![1614856173085](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856173085.png)

![1614856298363](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856298363.png)

![1614856384338](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856384338.png)

这里每个词只有一个向量x，是u和v的和，之前Glove和word2vec中每个词都有两个向量u和v。

![1614856698461](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856698461.png)

![1614856758048](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856758048.png)

![1614856927756](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856927756.png)

![1614856973031](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614856973031.png)

拼接成一个矩阵，用一次乘法完成，速度相差了将近12倍。

![1614857292048](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857292048.png)

![1614857347022](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857347022.png)

![1614857398070](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857398070.png)

![1614857494869](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857494869.png)

![1614857512457](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857512457.png)

![1614857628232](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857628232.png)

![1614857677669](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857677669.png)

![1614857745275](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857745275.png)

![1614857802067](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857802067.png)

![1614857977899](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614857977899.png)

![1614858051816](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614858051816.png)

![1614858098625](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614858098625.png)

将之前的softmax换成单层神经网络，输入x仍然是5d的窗口。

![1614858591111](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614858591111.png)

可以看出，每个词4维，所以x是20维，隐藏单元有8个，所以W是8x20，

![1614858790348](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614858790348.png)

以前只能学习in出现在这里，会增加下一个词是地点的概率，现在可以学习如果in在第二个位置，而且museums在第一个位置时，会提高中心词是地点的可能性。可以学习不同输入之间的相互作用。

![1614859041733](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614859041733.png)

如果训练示例是正确的，或者s>s_c，损失J=0，目的是minimize J

![1614859469312](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614859469312.png)

![1614859833032](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614859833032.png)

s对W的求导结果就是a。

对W_ij求导只需要使用a_i。

![1614860338541](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614860338541.png)

应用链式法则求导

![1614860612669](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614860612669.png)

![1614860714878](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614860714878.png)

![1614860941651](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1614860941651.png)

在高层重复使用已经求得的导数，可以提高效率。

![1615014412069](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615014412069.png)

![1615014559428](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615014559428.png)

---

## 第五讲 

反向传播和项目建议

![1615015537012](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615015537012.png)

![1615016044207](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615016044207.png)

![1615017602292](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615017602292.png)

![1615019159372](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615019159372.png)

---

## 第六讲

![1615020713646](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615020713646.png)

![1615021931050](C:\Users\lizhuoran\AppData\Roaming\Typora\typora-user-images\1615021931050.png)




















