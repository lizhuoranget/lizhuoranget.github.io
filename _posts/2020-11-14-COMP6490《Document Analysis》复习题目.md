## è®¡ç®—

1. TF-IDFã€cos
2. recallã€precisionã€F1-Score
3. HITS and PageRank 
4. Matrix
5. derivative
6. PMI
7. Backprop
8. attention
9. transformer
10. K-means
11. derivation for the string
12. language model probality
13. Nivreâ€™s arc-eager parser 

## 3. IR

#### Q1  Pre-processing 

**(a)** **List all of the main steps used in IR system pre-processing in the order they are performed. Briefly describe each step.** 

Answer: 

Tokenization, stop word removal, normalisation 

**(b) Describe the difference between lemmatization and stemming.** 

Answer: 

Stemming always removes some portion from the end of words, lemmatization can replace words entirely e.g. good -> better. Usually, lemmatization takes longer to compute. 

**(c) Apart from whitespace tokenization, come up with 2 other methods of tokenization. Discuss why each of your proposed tokenization methods might be useful for the task of IR.**

Answer: 

Tokenize based on grammar (e.g. treat all of â€˜,.!?â€™ As whitespace). 

Use regexp to parse numbers (e.g. 1,000) into a single token. 



#### Q2: Querying 

Consider the following term-document matrix for 3 terms "quick","brown","fox" in a collection of 3 documents:

![image-20201112103524204](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112103524204.png)

Tf = è¯¥è¯åœ¨æ–‡ç« å‡ºçŽ°çš„æ¬¡æ•°

IDF = logï¼ˆæ–‡æ¡£æ€»æ•°/åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°ï¼‰

TF-IDF = TF * IDF

**(a) calculate the tf-idf score**

![image-20201112103631997](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112103631997.png)

**(b)Now suppose that a user runs the query â€œquick foxâ€. Calculate the cosine similarity between this query and each of the 3 documents, where the document and query vectors are given by the tf-idf score of each term. Which document is retrieved first?** 

$tf-idf_q$ = [log3, 0 ,0]

cos = AB/(|A||B|)

Answer: 

Sim(query, Doc1) = log3 * 3log3 / (log3 * 3log3) = 1 

Sim(query, Doc2) = 0 

Sim(query, Doc3) = 0 

1 is retrieved first. 

**(c)** **Write down the inverted index that would be created from this term-document matrix as a Python dictionary.** 

Answer: 

{â€œquickâ€: [(Doc1, 3)], 

â€œbrownâ€: [(Doc2, 1), (Doc3, 3)], 

â€œfoxâ€: [(Doc1, 2), (Doc2, 1), (Doc3, 6)]} 

**(d)** **Explain the importance of the idf component of the tf.idf score. How does the idf change the weights of rare terms and why is this useful in information retrieval?** 

Answer: 

The IDF term weights rare words higher, whereas words that appear in lots of documents are given small weights. This is useful for IR because a word that appears in only a few documents is likely related to the topic of those documents, which means they will inform the query results more. 



#### Q3: Evaluation 

Suppose that we are evaluating our IR system. For a given query, our system retrieves 10 documents, which are marked as being relevant (R) or irrelevant (I) in the following order: 

R, I, R, R, I, R, R, R, R, R 

The list is ordered left to right, so the leftmost R is the relevance of the first retrieved document. There are 12 relevant documents in the entire collection. 

tp = 8, fp = 2, fn = 4, tn = 

**(a)** **Calculate the recall at 5 documents retrieved.** 

Tp = 3,fp=2,fn = 9

recall = tp/(tp+fn)

Precision = tp/(tp+fp)

Accuracy = (tp+tn)/(tp+tn+fp+fn)

Recall = 3/12

P = 3/5

**(b) Calculate the interpolated precision at 20% recall.** 



**(c)** **Calculate the F1-score at 5 documents retrieved** 

F1 = 2PR/(P+R)

F1 = 0.35294118

**(d)** **Consider the task of building an IR system for a collection of legal documents (patents, court transcripts, etc) to be used by legal firms. How would you evaluate this IR system? Compare the differences in user needs between this system and a typical web search application. How would these differences influence what metrics you use to measure your system performance?** 

Answer: 

For web search, users just want to find some relevant results as quickly as possible and are unlikely to look through more than 5 retrieved documents. A lawyer preparing for a court case might need to find some contract or other important document relevant to the case. Because of this they would be willing to spend more time looking through results as it is very important that they find the specific document they are looking for. Therefore, recall@100 would be a good measure for the system, as it captures how well the system can eventually find relevant documents. 

## 4. Web Search & Linear Regression

####  Q1: HITS and PageRank 

**(a)** **Give 2 examples of differences between bibliographic references and webpage links and explain how these differences affect IR algorithms for each.** 

Answer: 

1. Many webpage links are purely navigational. 

2. Many webpages with high in-degree are portals not content providers. 



This means that web search algorithms should try and separate content providers from hubs and ignore links within the same website. 

**(b)** **Given the following graph which describes hyperlinks between 5 web-pages:** 

 **Run the HITS algorithm for two iterations. What are the authority and hub scores for each website?**  

![image-20201110152132067](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201110152132067.png)

Answer: 

a0 = [1, 1, 1, 1, 1], h0 = [1, 1, 1, 1, 1] 

a1 = [1, 1, 1, 2, 1] / sqrt(8), h1 = [3, 1, 1, 1, 0] / sqrt(12) 

a2 = [1, 3, 3, 2, 3] / (sqrt(12) * sqrt(32/12)), h2 = [3, 2, 2, 1, 0] / (sqrt(8) * sqrt(18/8)) 

**(c)** **Using the same graph from question (b), run the Page Rank algorithm for two iterations. What are the authority scores for each website?** 

Answer: 

r0 = [1, 1, 1, 1, 1] / 5 

r1 = [1/5, 1/15, 1/15, 2/5, 1/15] / (12/15) 

r2 = [1/2, 1/12, 1/12, 1/6, 1/12] / (11/12) 

#### Q2: One-hot Vectors 

**Suppose we have three words in our language: â€œquickâ€, â€œbrownâ€, â€œfoxâ€. Compute one-hot vector representations for each of these 3 words.** 

Answer: 

â€œquickâ€ = [1 0 0] 

â€œbrownâ€ = [0 1 0] 

â€œfoxâ€ = [0 0 1] 

#### Q3: Matrix Algebra 

**(a)** **Compute AB, where ð´=[2 âˆ’1 0 1] and ðµ=[âˆ’1 3 1 âˆ’2]** 

Answer: [âˆ’3 8 1 âˆ’2] 

**(b)** **Compute â€–ð‘¥âˆ’ð‘¦â€–2, where ð‘¥=[2âˆ’1], ð‘¦=[âˆ’13]** 

Answer: 5 

**(c)** **Compute ð‘‘ð¿ð‘‘ð‘Š where ð‘Š=[ð‘¤1ð‘¤2], ð‘¥=[2âˆ’1], ð¿=12(ð‘Šð‘¥âˆ’1)2** 

Answer: ð‘‘ð¿ð‘‘ð‘¤1=(ð‘Šð‘¥âˆ’1)2,ð‘‘ð¿ð‘‘ð‘¤2=(ð‘Šð‘¥âˆ’1)(âˆ’1) 

#### Q4: Multiple Linear Regression 

**(a)** **Suppose we are trying to build a linear model which predicts 3 output variables for each 1-dimensional input data point. One approach would be to train one [3,1] weight matrix which maps inputs to a vector output. Another approach could be to train 3 separate [1,1] weight matrixes, one for each of the target variables. Which one of these methods would you expect to perform better and why?** 

Answer: They are exactly the same. 

#### Q5: Adding more features 

**When using linear regression, we must represent our input objects as a vector. Each component of the vector is a feature which describes something about the input (e.g. for TF vectors, each component tells the model how many times a particular word occurs in the document). Is it possible that adding more features to a dataset, that is giving more information about each object to the model, causes a model to make worse predictions? Justify why you believe this is possible or not possible.** 

Answer: It is possible, consider the following 2 datasets, where the true relationship is ð‘¦=ð‘¥1+0ð‘¥2 (with some random noise). 

D1 = {([0,0],-1), ([0,0],1), ([1,0],0), ([1,0],2)} 

D2 = {([0,0],-1), ([0,1],1), ([1,0],0), ([1,1],2)} 

Solving for the line with the least squared error on D1 gives ð‘¦=ð‘¥1+0ð‘¥2, but on D2 gives ð‘¦=ð‘¥1âˆ’2ð‘¥2. Even though there is no relationship between ð‘¥2 and ð‘¦, if we introduce ð‘¥2 into the dataset, the model may learn a relationship that does not exist due to noise in the dataset. This is known as overfitting. 

#### Q6: Cross-Entropy Loss (Challenge question) 

**Suppose we are training a linear classifier. For some training datapoint we have ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)=[0.10.9] ð‘¦=[10]** 

**(a) Compute the derivative of the loss of this datapoint with respect to the first bias ð‘‘ð¿ð‘‘ð‘1, where the loss is mean squared error ð¿=Î£12(ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)ð‘–âˆ’ð‘¦ð‘–)22ð‘–=1.** 

Answer: 

Let ð‘§=ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘), then ð‘‘ð¿ð‘‘ð‘1=ð‘‘ð¿ð‘‘ð‘§1ð‘‘ð‘§1ð‘‘ð‘1+ð‘‘ð¿ð‘‘ð‘§2ð‘‘ð‘§2ð‘‘ð‘1 ð‘‘ð¿ð‘‘ð‘§1=(z1âˆ’y1) 

ð‘‘ð¿ð‘‘ð‘§2=(z2âˆ’y2) ð‘‘ð‘§1ð‘‘ð‘1=ð‘‘exp((ð‘Šð‘¥+ð‘)1)exp((ð‘Šð‘¥+ð‘)1)+exp((ð‘Šð‘¥+ð‘)2)ð‘‘ð‘1 ð‘‘ð‘§1ð‘‘ð‘1=exp((ð‘Šð‘¥+ð‘)1)exp((ð‘Šð‘¥+ð‘)1)+exp((ð‘Šð‘¥+ð‘)2)âˆ’exp((ð‘Šð‘¥+ð‘)1)2(exp((ð‘Šð‘¥+ð‘)1)+exp((ð‘Šð‘¥+ð‘)2))2 =ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)1(1âˆ’ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)1) ð‘‘ð‘§2ð‘‘ð‘1=ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)2(1âˆ’ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)2) 

**(b) Recompute (a) but this time use cross-entropy loss instead of mean squared error, ð¿=Î£log (ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)ð‘–)ð‘¦ð‘–2ð‘–=1=log (ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)1)** 

Answer: ð‘‘ð¿ð‘‘ð‘1=ð‘‘log(ð‘§1)ð‘‘ð‘§1ð‘‘ð‘§1ð‘‘ð‘1 ð‘‘log(ð‘§1)ð‘‘ð‘§1=1ð‘§1 ð‘‘ð‘§1ð‘‘ð‘1=ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)1(1âˆ’ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð‘Šð‘¥+ð‘)1) 

**(c) If we were to train our model using one of these losses, which of these two losses do you think would result in a better model?** 

Answer: 

The cross-entropy derivative can be arbitrarily large, while the MSE gradient is the sum of two values bounded between 0 and 1, so the cross-entropy derivative should allow for much faster training in general. 

```
for the cross-entropy, the highest error between predict and label, the faster the decline
```

## 5. Embedding_DNN

#### Q1: Pairwise Mutual Information Vectors 

**Given the following word cooccurrence matrix** 

 **(a)** **Compute the pairwise mutual information representation vector of the words â€œquickâ€, â€œbrownâ€, â€œfoxâ€.** 

![image-20201109164150737](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201109164150737.png)

Answer: 

Quick = [log((4/30)/((1/3)*(1/3))), log((3/30)/((1/3)*(12/30))), log((3/30)/((1/3)*(8/30)))] 

Brown = [log((4/30)/((11/30)*(1/3))), log((6/30)/((11/30)*(12/30))), log((1/30)/((11/30)*(8/30)))] 

Fox = [log((2/30)/((9/30)*(1/3))), log((1/10)/((9/30)*(12/30))), log((4/30)/((9/30)*(8/30)))] 

 **(b)** **When calculating pairwise mutual information, we take the logarithm of the ratio of probabilities. Describe what effect applying the logarithm has on the vector representations.** 

Answer: It reduces the size of the components 

```
It can reduce the value of ratio largely.
```

#### Q2: Word2vec 

**(a)** **Given the following sentence: â€œThe quick brown fox jumpsâ€, we wish to perform word2vec embedding with a window size of 3 to create a representation vector of each word. Transform this sentence into a training dataset that we can train our embedding model on. Your dataset should have an input vector and a target vector for each context in the sentence.** 

 Answer: 

D = {([the brown], quick), 

([quick fox], brown), 

([brown jumps], fox)} 

 **(b)** **In your own words explain why word2vec models tend to learn vector representations which obey simple arithmetic operations, e.g. king â€“ man + woman = queen.** 

Answer: The word2vec model tries to predict a missing word based on its context using a *linear* function. In order for it to make accurate predictions, then it must be able to encode semantic meaning of words into *linear* operations on vectors. 

```
Kings and queens often appear together, and men and women also appear together, which belong to association words. Therefore, the regular and stable position of a word is usually determined by its combination words, which are words that often appear together. So these most frequently occurring combination words determine the update rule of the target word.
```

**(c)** **If you replaced the linear predictor in word2vec with a 1 hidden layer neural network, would the model still learn vector representations which obey simple arithmetic operations? Would the learned vector representations be more useful for other models?** 

Answer: No, with a NN predictor the meaning of words could, and probably would, be encoded in complicated non-linear operations. 

#### Q3: Backprop 

![image-20201112211559253](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112211559253.png)

#### Q4: Multiple Outputs 

**(a)** **Suppose we are trying to build a neural network model which predicts 3 output variables for each 5-dimensional input data point. One approach would be to train one neural network which outputs a 3-dimensional vector. Another approach could be to train 3 separate neural networks, one for each of the target variables. Which one of these methods would you expect to perform better and why?** 

Answer: For the single neural network, its hidden layers will learn to create a representation of the input from which all 3 values can be predicted. With 3 different neural networks, each one can learn different representations. This will usually result in a regularizing effect on the single network, since it uses fewer parameters. 

å•ä¸ªç½‘ç»œå…·æœ‰ç›¸åŒçš„éšè—å±‚è¡¨ç¤ºï¼Œæ‰€ä»¥ç»“æžœæ›´æ­£è§„ã€‚

#### Q5: Generalization 

**(a)** **Consider a 1 hidden layer neural network. Describe what effect the number of hidden neurons has on the modelâ€™s predictive ability**. 

Answer: For a neural network to perform well, it must have enough hidden neurons to be able to approximate the true distribution of the data. However, if a neural network has many hidden neurons then it can end up learning noise (small random variations) from the training dataset and hence make worse predictions than one with less neurons. 

æ¨¡åž‹éœ€è¦å¤§é‡ç¥žç»å…ƒï¼Œä½†æ˜¯è¿‡å¤šä¼šå¼•èµ·è¿‡æ‹Ÿåˆã€‚

**(b)** **Describe what effect the activation function has on the modelâ€™s predictive ability.** 

Answer: The output of a neural network is made by scaling, shifting, and flipping the activation function. For a given dataset, some activation functions may be better suited than others, but it is usually difficult to know which, hence simple activation functions (such as relu) are preferred.

ä¸åŒçš„è®­ç»ƒé›†ä½¿ç”¨ä¸åŒçš„æ¿€æ´»å‡½æ•°å¯èƒ½æ•ˆæžœä¼šå¾ˆå¥½ï¼Œä½†æ˜¯å¾ˆéš¾åŽ»é€‰æ‹©ï¼Œæ‰€ä»¥ä¸€èˆ¬ä½¿ç”¨ç®€å•æœ‰æ•ˆçš„ã€‚ 

**(c)** **Consider a one hidden layer neural network with 10 hidden neurons and a two hidden layer network with 5 and 2 hidden neurons. Both networks have 1 dimensional input. Which of these networks has a greater representation capacity (that is, will be able to represent the most complicated functions)? In general, how does the capacity of a network scale with the number of layers?** 

Answer: Consider the 2 layer network. Each neuron in the second layer can be though of as a 1 layer network with 5 hidden neurons. Then the output of the network is a linear combination of 2 5 hidden neuron networks, which is the same as one layer of 10 neurons. This ignores the fact that for the 2 layer network, the second layer neurons have to use the *same* hidden neurons, so actually it will have slightly less capacity. In general, assuming that each layer has more neurons than the input dimension, capacity grows exponentially in depth. 

2*5å¾®å¼±ã€‚å¦‚æžœç¥žç»å…ƒä¸ªæ•°å¤§äºŽè¾“å…¥ç»´åº¦ï¼Œåˆ™å±‚æ•°å¢žåŠ èƒ½åŠ›å¢žå¼ºå¾ˆå¤šã€‚

#### Q6: Early Stopping 

**Neural networks are almost always trained with â€œearly stoppingâ€, this means that during training the network is regularly evaluated on a held-out *validation* dataset, and if the performance on the *validation* set does not increase then the training process is stopped early.** 

**(a)** **Do you think that early stopping would improve or decrease a neural networkâ€™s predictive performance?** 

Answer: Prevents overfitting, improving performance. 

æå‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

**(b)** **If you split your dataset into 2 parts, train and test, and used the test set to decide when to stop training, do you think that the test accuracy would still be a good estimation of how the network will perform on new unseen data?** 

Answer: No, since we have specifically optimized the network to perform well on this test set. 

å¦ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¯¹è¿™ä¸ªæµ‹è¯•é›†è¿›è¡Œäº†ä¼˜åŒ–ã€‚

**(c)** **If we are using early stopping, how do you think the test accuracy will change as we increase the number of hidden neurons?** 

Answer: With early stopping, performance to increase as the number of hidden neurons increases, seemingly without limit. 

æ€§èƒ½éšç€ç¥žç»å…ƒæ•°ç›®å¢žåŠ ï¼Œä¼¼ä¹Žæ²¡æœ‰é™åˆ¶ã€‚

## 6. RNN

####  Q1: Recurrent models 

**In your own words explain why recurrent models are needed to process sequential data.** 

Answer: Sequences can have arbitrary length. 

ç”±äºŽåºåˆ—é•¿åº¦å¯ä»¥ä»»æ„ã€‚

#### Q2: Parameter counts 

**a) For a simple RNN cell with input dimension ð‘– and hidden dimension â„Ž, how many parameters (weights and biases) does this cell have?** 

Answer: (i+h)*h + h 

**b) How many parameters does a GRU cell with input dimension ð‘– and hidden dimension â„Ž have?** 

Answer: 3 * ((i+h)*h + h) 

**c) How many parameters does a LSTM cell with input dimension ð‘– and hidden dimension â„Ž have?** 

Answer: 4 * ((i+h)*h + h) 

#### Q3: Vanishing and exploding gradients 

**For this question we will attempt to understand why gradients vanish and explode in RNNs. To make the calculations simpler we will ignore the input vector at each timestep as well as the bias, so the update equation is given by** 

![image-20201112214722690](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112214722690.png)

![image-20201112214753112](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112214753112.png)

**b) Explain why the value of this derivative will become very small (vanish) or very large (explode) as the number of time-steps ð‘¡ increases.** 

Answer: Ignoring the activation function, this derivative is proportional to ð‘Šð‘¡. If the weights in W are less than 1 this quantity shrinks exponentially, if they are greater than 1 it grows exponentially. 

æ¢¯åº¦ä¸Žæƒé‡Wæˆæ¯”ä¾‹ï¼Œå¤§äºŽ1æŒ‡æ•°å¢žé•¿ï¼Œå°äºŽ1æŒ‡æ•°æ”¶ç¼©

**c) Explain why vanishing and exploding gradients mean that the model will â€œforgetâ€.** 

Answer: During training, gradients of one time-step w.r.t to previous time-steps will be either small or large, meaning weights will either be not changed or not converge. This un-stable training means that previous hidden states will not learn to be useful for the current step. 

å› ä¸ºå¦‚æžœæ¢¯åº¦å¾ˆå¤§æˆ–å¾ˆå°ï¼Œåˆ™æƒé‡å¯èƒ½ä¸ä¼šæ”¹å˜ï¼Œæ‰€ä»¥éšè—å±‚æ— æ³•å­¦ä¹ ã€‚

**d) In our calculations we ignored the inputs at each step and the bias. If we use the full update equation â„Žð‘¡+1 = ðœ™(ð‘Š1â„Žð‘¡ + ð‘Š2ð‘¥ð‘¡ + ð‘), do our conclusions about vanishing and exploding gradients still hold?** 

Answer: Yes, because the gradient is still proportional to ð‘Šð‘¡. 

**e) We have seen that the LSTM and GRU cells can help to avoid exploding and vanishing gradients. Can you think of any other ways to change a simple RNN in order to increase its memory?** 

(1) Previous output of  t-1 timestep as  memory content to the input of  t titimestep

(2) The bidirectional RNN is composed of a forward RNN and a reverse RNN. The forward RNN reads the input sequence from front to back, and the reverse RNN reads the input sequence from back to forward. The network output at each moment is composed of the output of the forward RNN and The output of the reverse RNN is jointly determined

## 7. attention

#### Q1: Encoder-Decoder models 

**Weâ€™ve seen how encoder-decoder architectures can be used for natural language translation. Give another example of a task that encoder-decoder models would be useful for.** 

Answer: Any task where the output is some type of data structure, e.g. translating code from one programming language to another, semantic parsing, predicting how a network (graph) will change over time. 

è¾“å‡ºä¸ºæŸç§æ•°æ®ç»“æž„ç±»åž‹çš„ä»»ä½•ä»»åŠ¡

#### Q2: Attention practice 

**Given a set of key vectors {ð‘˜1=[2âˆ’13],ð‘˜2=[124],ð‘˜3=[âˆ’2âˆ’30]} and query vector ð‘ž=[1âˆ’12], compute the attention weighted sum of keys using dot-product similarity ð‘ ð‘–ð‘š(ð‘¥,ð‘¦)=ð‘¥ð‘‡ð‘¦** 

ð‘ŽÌƒ = [k*q]

$a = e^ð‘ŽÌƒ_i/sum(e^ð‘ŽÌƒ)$

Answer: ð‘ŽÌƒ=[9,7,1], ð’‚=[0.88,0.12,0],ð‘¦=0.88[2âˆ’13]+0.12[124]+0[âˆ’2âˆ’30]=[1.88âˆ’0.643.12] 

####  Q3: Similarity scores 

**A scoring function is used to measure similarity between two vectors, which then determines the attention weight between query and key. Compare and contrast each of the different scoring functions given in the lecture slides. How do you think these differences would affect an attention model?** 

Answer: 

The scaled dot-product attention makes the expected size of the scores independent of the vector dimension, which leads to more stable gradients across layers. ð‘¥ð‘‡ð‘Šð‘¦ allows for different dimensions to contribute different amounts to the resulting score. ðœˆð‘‡tanhâ¡(ð‘Š[ð‘¥,ð‘¦]) is training a 1 hidden layer neural network to predict the similarity of the two vectors x and y, it can learn a non-linear similarity function which allows for more expressivity than the simple dot product similarities, at the cost of more parameters.

ç¼©æ”¾ç‚¹ç§¯ä½¿å¾—åˆ†çš„æœŸå¾…å¤§å°ç‹¬ç«‹äºŽå‘é‡ç»´åº¦ï¼Œæ¢¯åº¦æ›´åŠ ç¨³å®šã€‚

 ð‘¥ð‘‡ð‘Šð‘¦ å…è®¸ä¸åŒç»´åº¦å¯¹ç»“æžœè´¡çŒ®ä¸åŒã€‚

ðœˆð‘‡tanhâ¡(ð‘Š[ð‘¥,ð‘¦]) ä½¿ç”¨1å±‚éšè—å±‚é¢„æµ‹ç›¸ä¼¼æ€§ï¼Œå¯ä»¥å­¦ä¹ éžçº¿æ€§çš„ç›¸ä¼¼æ€§å‡½æ•°ï¼Œè¡¨è¾¾æ›´å¼ºï¼Œè¦ç”¨æ›´å¤šçš„å‚æ•°ã€‚

#### Q4: Distance scores 

**Usually attention weights are computed based on similarity between query and key. Would it be possible to instead compute attention weights based on the Euclidean distance between query and key, so that vectors which are further apart have smaller weight? If so, then explain how you would do it. If not, then explain what trouble you would run into.** 

Answer: Usually neural network hidden spaces tend to encode meaning as directions in the space, so if you have 2 vectors which both pointed in the same direction but one was much longer, they would have a large distance between them but both would tend to represent the kind of input. For this reason, distances tend to not work as well. 

ä¸èƒ½ï¼Œè·ç¦»ä»£æ›¿ç›¸ä¼¼åº¦ï¼Œç›¸ä¼¼ç±»åž‹çš„è¡¨ç¤ºä¼šæœ‰å¾ˆå¤§è·ç¦»ã€‚

#### Q5: SoftMax normalisation 

**a) Attention weights are run through a SoftMax function to ensure they sum up to 1. Explain why this step is important.** 

Answer: The SoftMax ensures that the weights add up to 1, which means the weights are split among the items. This forces the model to focus its attention, since having a large weight for one item means smaller weights for the others. 

å¼ºåˆ¶è®©æ¨¡åž‹æ³¨æ„åˆ°æŸéƒ¨åˆ†ï¼Œå¤§çš„æƒé‡æ„å‘³ç€å…¶ä»–å°±æ˜¯å°æƒé‡ã€‚

**b) Would the attention mechanism still work if we did the normalisation as follows?** 

ð’‚ð‘–=ð’“ð’†ð’ð’–(ð’‚Ìƒð’Š)Î£ð’“ð’†ð’ð’–(ð’‚Ìƒð’‹)ð’ð’‹=ðŸ 

Answer: in principle it should still work, although you might run into issues if the denominator is 0. 

åŽŸåˆ™å¯ä»¥ï¼Œå°½ç®¡åˆ†æ¯å¯èƒ½ä¸º0.

#### Q6: Tree based Encoder-Decoder (Challenge Question) 

**Suppose that we have a dataset where both the inputs and outputs are binary trees. Each node in the tree contains a vector of features which represent that tree. The predicted trees should have the same structure as the target trees, and all of the predicted nodes should have the same vectors as the target nodes.** 

**a) Describe how you would design an encoder-decoder model for this dataset.** 

Answer: The encoder can be a RNN which maps a node and both of its children to a vector. This encoder can be applied to every node in the tree, beginning with the leaf nodes and working up to the root. The decoder would work in reverse, it maps a vector to 3 output vectors, the first is used to predict the item of the current node, and the other 2 are used for the children. 

```
è¯¥ç¼–ç å™¨å¯ä»¥åº”ç”¨äºŽæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œä»Žå¶èŠ‚ç‚¹å¼€å§‹ä¸€ç›´åˆ°æ ¹ã€‚è§£ç å™¨å°†åå‘å·¥ä½œï¼Œå®ƒå°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°3ä¸ªè¾“å‡ºå‘é‡ï¼Œç¬¬ä¸€ä¸ªç”¨äºŽé¢„æµ‹å½“å‰èŠ‚ç‚¹çš„é¡¹ç›®ï¼Œå…¶ä»–ä¸¤ä¸ªç”¨äºŽå­èŠ‚ç‚¹
```

**b) How would your model need to change if the trees were not binary, so that each node could have any number of children?** 

Answer: In the encoder we would need to apply the RNN across all of the child nodes first, before combining child and parent nodes together. Similarly, the decoder would decode all of the children nodes as a list. 

```
åœ¨ç¼–ç å™¨ä¸­ï¼Œåœ¨å°†å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ç»„åˆåœ¨ä¸€èµ·ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆåœ¨æ‰€æœ‰å­èŠ‚ç‚¹ä¸Šåº”ç”¨RNNã€‚åŒæ ·ï¼Œè§£ç å™¨ä¼šå°†æ‰€æœ‰å­èŠ‚ç‚¹è§£ç ä¸ºåˆ—è¡¨ã€‚
```

## 8. Transformer

####  Q1: Transformer datasets 

**Describe a type of task where a transformer model is likely to perform much better than a recurrent model.** 

Answer: Any task where long term dependency is needed, e.g. generating news articles, question answering, language translation. 

```éœ€è¦é•¿æœŸä¾èµ–çš„ä»»ä½•ä»»åŠ¡
éœ€è¦é•¿æœŸä¾èµ–çš„ä»»ä½•ä»»åŠ¡
```

#### Q2: Transformer complexity 

**a) What is the computational complexity (how many computation steps do you need to perform) for a self-attention block with vector dimension ð‘‘ and sequence length ð‘™** 

![image-20201112222330624](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112222330624.png)

**b) Is this complexity smaller or larger compared to a GRU with dimension ð‘‘ and sequence length ð‘™** 

Answer: For a GRU, each step only requires computing 3 gates, which is O(d\*d*l). The GRU has a lower complexity. 

**c) Hence the transformer model can be trained in much shorter time, True or False?** 

Answer: While the GRU needs to perform fewer operations, a transformer can perform all of its computation in parallel. So provided you have many processors to run on, a transformer can actually be computed faster (in less wall time). 

GRUæ“ä½œè™½ç„¶å°‘ï¼Œä½†æ˜¯transformerå¯ä»¥å¹¶è¡Œè®¡ç®—ç¼–ç å™¨ã€‚

#### Q3: Self-Attention practice 

![image-20201112222602053](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112222602053.png)

#### Q4: Feed-forward layers 

**The transformer architecture repeats self-attention layers and then feed-forward layers. Explain the importance of each of these layers.** 

Answer: The self-attention layers mix up the representations of each input word, each input word can attend to every other word in the input, regardless of distance, to determine what to include in its own representation. The feed-forward layers introduce non-linear processing of each wordâ€™s representation. 

```
æ¯ä¸ªè¾“å…¥å•è¯éƒ½å¯ä»¥ä¸Žè¾“å…¥ä¸­çš„æ¯ä¸ªå…¶ä»–å•è¯ç›¸å…³è”ï¼Œè€Œä¸è®ºè·ç¦»å¦‚ä½•ï¼ŒFFæ›¾å¼•å…¥äº†éžçº¿æ€§å¤„ç†ã€‚
```

#### Q5: Pre-training 

**Explain why neural network architectures are especially well suited for pre-training on un-labelled data.** 

Answer: Neurons in a neural network tend to learn useful representations of their input. The neurons in lower layers will often learn general features that are useful for a wide range of tasks, not just the one they are trained on. Additionally, neural networks can be trained iteratively, unlike some other machine learning models which need to be fit on the entire dataset. 

```
ç¥žç»ç½‘ç»œä¸­çš„ç¥žç»å…ƒå€¾å‘äºŽå­¦ä¹ å…¶è¾“å…¥çš„æœ‰ç”¨è¡¨ç¤ºã€‚è¾ƒä½Žå±‚çš„ç¥žç»å…ƒé€šå¸¸ä¼šå­¦ä¹ å¯¹è®¸å¤šä»»åŠ¡æœ‰ç”¨çš„ä¸€èˆ¬ç‰¹å¾ï¼Œè€Œä¸ä»…ä»…æ˜¯å¯¹å®ƒä»¬è¿›è¡Œè®­ç»ƒçš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç¥žç»ç½‘ç»œå¯ä»¥è¿­ä»£åœ°è®­ç»ƒï¼Œè¿™ä¸Žå…¶ä»–ä¸€äº›éœ€è¦é€‚åˆæ•´ä¸ªæ•°æ®é›†çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ä¸åŒ
```

#### Q6: Self-supervised objectives 

**a) In BERTâ€™s masked language modelling training objective, masked tokens are sometimes kept as the same word or replaced with a random word, instead of using the [MASK] token. Explain why this is done.** 

Answer: Always training with the [MASK] token means that the model will learn to expect [MASK] tokens in its input, however at test time there will not be any [MASK] tokens in the input. By sometimes training with regular words, the model learns to represent all of the words in the input. 

```
å§‹ç»ˆä½¿ç”¨[MASK]ä»¤ç‰Œè¿›è¡Œè®­ç»ƒæ„å‘³ç€è¯¥æ¨¡åž‹å°†å­¦ä¼šåœ¨å…¶è¾“å…¥ä¸­æœŸæœ›ä½¿ç”¨[MASK]ä»¤ç‰Œï¼Œä½†æ˜¯åœ¨æµ‹è¯•æ—¶ï¼Œè¾“å…¥ä¸­å°†æ²¡æœ‰ä»»ä½•[MASK]ä»¤ç‰Œã€‚é€šè¿‡æœ‰æ—¶ä½¿ç”¨è§„åˆ™çš„å•è¯è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡åž‹å­¦ä¼šè¡¨ç¤ºè¾“å…¥ä¸­çš„æ‰€æœ‰å•è¯
```

**b) Explain why for BERTâ€™s next sentence prediction task, inputs are encoded as [CLS] sentence1 [SEP] sentence2 [SEP].** 

Answer: The [CLS] token lets the model know that it is performing classification (and not trying to fill in [MASK] missing words), and the [SEP] token lets the model know where one sentence ends and the next begins. 

```
[CLS]ä»¤ç‰Œä½¿æ¨¡åž‹çŸ¥é“å®ƒæ­£åœ¨æ‰§è¡Œåˆ†ç±»ï¼ˆè€Œä¸æ˜¯å°è¯•å¡«å……[MASK]ä¸¢å¤±çš„å•è¯ï¼‰ï¼Œè€Œ[SEP]ä»¤ç‰Œä½¿æ¨¡åž‹çŸ¥é“ä¸€ä¸ªå¥å­åœ¨å“ªé‡Œç»“æŸè€Œä¸‹ä¸€å¥è¯åœ¨å“ªé‡Œå¼€å§‹ã€‚
```

## 9. clustering and semantics

#### Q1: Unsupervised Learning 

**a) Discuss the differences between supervised learning, self-supervised learning and unsupervised learning.** 

Answer: Supervised learning has human provided labels, self-supervised has labels derived from the raw data itself, unsupervised learning uses no labels what-so-ever. 

```
ç›‘ç£å­¦ä¹ å…·æœ‰äººå·¥æä¾›çš„æ ‡ç­¾ï¼Œè‡ªæˆ‘ç›‘ç£å…·æœ‰æºè‡ªåŽŸå§‹æ•°æ®æœ¬èº«çš„æ ‡ç­¾ï¼Œæ— ç›‘ç£å­¦ä¹ åˆ™æ ¹æœ¬ä¸ä½¿ç”¨æ ‡ç­¾ã€‚
```

**b) Does unsupervised learning require a human to provide any additional information, or does it only require a set of data points?** 

Answer: Unsupervised learning typically requires some additional information about the structure of the data space, for example clustering algorithms require the user to specify a distance metric. 

```
æ— ç›‘ç£å­¦ä¹ é€šå¸¸éœ€è¦ä¸€äº›æœ‰å…³æ•°æ®ç©ºé—´ç»“æž„çš„é™„åŠ ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œèšç±»ç®—æ³•è¦æ±‚ç”¨æˆ·æŒ‡å®šè·ç¦»åº¦é‡ã€‚
```

#### Q2: Types of Clustering 

**a) Discuss the differences between flat and hierarchical methods.** 

Answer: Flat clustering methods partition all of the data points into a fixed number of clusters and iteratively update the clusters. (Agglomerative) hierarchical methods begin with every point in its own cluster and then pick 2 two closest clusters to merge together and continue merging in the manner. 

```
å¹³é¢èšç±»æ–¹æ³•å°†æ‰€æœ‰æ•°æ®ç‚¹åˆ’åˆ†ä¸ºå›ºå®šæ•°é‡çš„ç¾¤ï¼Œå¹¶è¿­ä»£æ›´æ–°ã€‚ åˆ†å±‚èšç±»æ–¹æ³•ä»Žå…¶è‡ªèº«ç¾¤é›†ä¸­çš„æ¯ä¸ªç‚¹å¼€å§‹ï¼Œç„¶åŽé€‰æ‹©2ä¸ªä¸¤ä¸ªæœ€æŽ¥è¿‘çš„ç¾¤é›†ä»¥åˆå¹¶åœ¨ä¸€èµ·å¹¶ç»§ç»­ä»¥è¿™ç§æ–¹å¼åˆå¹¶
```

**b) Discuss the differences between soft and hard clustering.** 

Answer: Hard clustering assigns each point to at most one class, soft clustering assigns each point a portion (or probability) of membership to each cluster. 

```
ç¡¬èšç±»å°†æ¯ä¸ªç‚¹æœ€å¤šåˆ†é…ç»™ä¸€ä¸ªç±»åˆ«ï¼Œè½¯èšç±»å°†æ¯ä¸ªç‚¹åˆ†é…ç»™æ¯ä¸ªèšç±»ä¸€éƒ¨åˆ†æ¦‚çŽ‡
```

**c) Propose how you could adjust the K-means algorithm to give soft clusters instead of hard clusters.** 

Answer: We could assign soft membership scores based on the distance from a point to each of the clusters, the probability of point i belonging to cluster j would be given by 1âˆ’ð‘‘(ð‘¥ð‘–,ð‘ð‘—)Î£ð‘‘(ð‘¥ð‘–,ð‘ð‘˜)|ð¶|ð‘˜=1 

```
æˆ‘ä»¬å¯ä»¥åŸºäºŽä»Žç‚¹åˆ°æ¯ä¸ªèšç±»çš„è·ç¦»æ¥åˆ†é…è¯„åˆ†
```

#### Q3: K-means Clustering Practice 

**Given the following set of 1 dimensional points: 1, 3, 6, 8, 20** 

**a) Run 2 iterations of k-means with Euclidean distance, k=2, and initial centres 1, 20.** 

Answer: 

Iteration 1 

Assignment: [1, 3, 6, 8], [20] 

Updated centres: [4.5], [20] 

Iteration 2 

Assignment: [1, 3, 6, 8], [20] 

Updated centres: [4.5], [20] 

**b) Run 2 iterations of k-means with Euclidean distance, k=3, and initial centres 1, 10, 20.** 

Answer: 

Iteration 1 

Assignment: [1, 3], [6, 8], [20] 

Updated centres: [2], [7], [20] 

Iteration 2 

Assignment: [1, 3], [6, 8], [20] 

Updated centres: [2], [7], [20] 

**c) Does k=2 or k=3 result in better clusters? Why?** 

Answer: k=3 seems to give a better clustering, because k=2 with only 2 clusters we have very uneven clusters, with one containing 4 points and the other only containing one point (due to the outlier). 

#### Q4: K-means Properties 

**For each of the following statements, decide whether they are true or false:** 

**a) K-means is guaranteed to converge to the globally best solution after a finite number of iterations.** 

Answer: False, it will converge but to a *locally* optimal solution. 

```
å®ƒä¼šæ”¶æ•›ï¼Œä½†ä¼šæ”¶æ•›åˆ°â€œå±€éƒ¨â€æœ€ä¼˜è§£
```

**b) The computational complexity of one iterations of k-means is ð‘‚(ð‘›ð‘˜2ð‘‘), where n is the number of data points and d is the dimension of the data points.** 

Answer: False, it should be ð‘‚(ð‘›ð‘˜ð‘‘). 

#### Q5: Purity 

**Suppose that you have run a clustering algorithm and found the following clusters: {ð‘¥1,ð‘¥2ð‘¥4 },{ð‘¥3,ð‘¥5}, and you additionally know that points are assigned to classes as follows: {ð‘¥1,ð‘¥2,},{ð‘¥3,ð‘¥4,ð‘¥5}** 

![image-20201110192111122](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201110192111122.png)

**a) Compute the purity between the assigned clusters and classes.** 

purity= 1/sum * max{äº¤é›†} * max{äº¤é›†}

Answer: 0.2 * 2 * 2 = 0.8 

**b) When used in this manner, is purity an intrinsic or extrinsic evaluation measure?** 

Answer: Extrinsic, because it uses external class labels to compare the clusters to. 

```
å¤–åœ¨çš„ï¼Œå› ä¸ºå®ƒä½¿ç”¨å¤–éƒ¨ç±»æ ‡ç­¾
```

#### Q6: Semantic Parsing 

**a) Give an example of an NLP task where using semantic parsing would be useful.** 

Answer: Any task where understanding semantic intent behind sentences is useful, e.g. machine translation, question answering, automatic code generation. 

```
ç†è§£å¥å­èƒŒåŽçš„è¯­ä¹‰æ„å›¾çš„ä»»ä½•ä»»åŠ¡éƒ½æ˜¯æœ‰ç”¨çš„ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œé—®é¢˜è§£ç­”ï¼Œè‡ªåŠ¨ä»£ç ç”Ÿæˆ
```

#### Q7: Embeddings 

**a) Discuss the differences between distributional and distributed embeddings.** 

Answer: Distributional embeddings are computed from context statistics, distributed embeddings represent meaning by numerical vectors, rather than symbolic structures. 

```
åˆ†å¸ƒåµŒå…¥æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡ç»Ÿè®¡æ•°æ®è®¡ç®—å¾—å‡ºçš„ï¼Œåˆ†å¸ƒåµŒå…¥é€šè¿‡æ•°å­—çŸ¢é‡è€Œä¸æ˜¯ç¬¦å·ç»“æž„æ¥è¡¨ç¤ºå«ä¹‰
```

#### Q8: Reference Resolution 

**For each of the following reference resolution models, briefly describe the model in terms of its inputs and key parameters.** 

**a) Mention-pair models** 

Answer: The model takes as input the embeddings of two mentions and outputs a binary decision, indicating whether or not they co-refer. 

```
è¯¥æ¨¡åž‹å°†ä¸¤ä¸ªæåŠçš„åµŒå…¥å†…å®¹ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªäºŒè¿›åˆ¶å†³ç­–ï¼Œè¡¨æ˜Žå®ƒä»¬æ˜¯å¦å…±åŒå¼•ç”¨
```

**b) Mention ranking models** 

Answer: The model takes as input the embedding of a single mention, and outputs a softmax probability distribution over every previous mention. 

```
è¯¥æ¨¡åž‹å°†å•ä¸ªæåŠçš„åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªå…ˆå‰æåŠçš„softmaxæ¦‚çŽ‡åˆ†å¸ƒã€‚
```

## 10. syntax parsing and language models

#### Question 1: Parsing Methods 

**Discuss the differences between constituency parsing and dependency parsing. Suggest an example application where constituency parsing would be preferred, and one where dependency parsing would be preferred.** 

è®¨è®ºæˆåˆ†å¥æ³•åˆ†æžï¼ˆconstituency parsing ï¼‰å’Œä¾å­˜å¥æ³•åˆ†æžï¼ˆ dependency parsingï¼‰åŒºåˆ«ã€‚

Answer: 

Dependency parsing can be more useful for several downstream tasks like Information Extraction or Question Answering. 

```
ä¾èµ–å…³ç³»è§£æžå¯¹äºŽä¸€äº›ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ä¿¡æ¯æå–æˆ–é—®é¢˜è§£ç­”ï¼‰å¯èƒ½ä¼šæ›´åŠ æœ‰ç”¨ï¼Œä¾‹å¦‚ï¼Œå®ƒå¾ˆå®¹æ˜“æå–é€šå¸¸æŒ‡ç¤ºè°“è¯ä¹‹é—´è¯­ä¹‰å…³ç³»çš„ä¸»è¯­-åŠ¨è¯-å®¾è¯­ä¸‰å…ƒç»„ã€‚å°½ç®¡æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»Žæˆåˆ†åˆ†æžæ ‘ä¸­æå–æ­¤ä¿¡æ¯ï¼Œä½†å®ƒéœ€è¦è¿›è¡Œå…¶ä»–å¤„ç†ï¼Œè€Œåœ¨ä¾èµ–å…³ç³»åˆ†æžæ ‘ä¸­ç«‹å³å¯ç”¨ã€‚
ä½¿ç”¨ä¾èµ–è§£æžå™¨å¯èƒ½ä¼šè¯æ˜Žæ˜¯æœ‰åˆ©çš„å¦ä¸€ç§æƒ…å†µæ˜¯ä½¿ç”¨è‡ªç”±å­—åºè¯­è¨€æ—¶ã€‚é¡¾åæ€ä¹‰ï¼Œè¿™äº›è¯­è¨€ä¸ä¼šå¯¹å¥å­ä¸­çš„å•è¯æ–½åŠ ç‰¹å®šçš„é¡ºåºã€‚ç”±äºŽåŸºç¡€è¯­æ³•çš„æ€§è´¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¾èµ–é¡¹è§£æžçš„æ€§èƒ½è¦å¥½äºŽæˆåˆ†ã€‚
```

For example, it makes it easy to extract subject-verb-object triples that are often indicative of semantic relations between predicates. Although we could also extract this information from a constituency parse tree, it would require additional processing, while itâ€™s immediately available in a dependency parse tree. 

Another situation where using a dependency parser might prove advantageous is when working with free word order languages. As their name implies, these languages donâ€™t impose a specific order to the words in a sentence. Due to the nature of the underlying grammars, dependency parsing performs better than constituency in this kind of scenario. 

On the other hand, when we want to extract sub-phrases from the sentence, a constituency parser might be better. 

```
å¦ä¸€æ–¹é¢ï¼Œå½“æˆ‘ä»¬æƒ³ä»Žå¥å­ä¸­æå–å­çŸ­è¯­æ—¶ï¼Œé€‰åŒºè§£æžå™¨å¯èƒ½ä¼šæ›´å¥½ã€‚
```

We can use both types of parse trees to extract features for a supervised machine learning model. The syntactical information they provide can be very useful for tasks like Semantic Role Labelling, Question Answering, and others. In general, we should analyse our situation and assess which type of parsing will best suit our needs. 

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸¤ç§ç±»åž‹çš„è§£æžæ ‘æ¥æå–ç›‘ç£åž‹æœºå™¨å­¦ä¹ æ¨¡åž‹çš„ç‰¹å¾ã€‚ä»–ä»¬æä¾›çš„è¯­æ³•ä¿¡æ¯å¯¹äºŽè¯¸å¦‚è¯­ä¹‰è§’è‰²æ ‡ç­¾ï¼Œé—®é¢˜å›žç­”ç­‰ä»»åŠ¡éžå¸¸æœ‰ç”¨ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬åº”è¯¥åˆ†æžæˆ‘ä»¬çš„æƒ…å†µå¹¶è¯„ä¼°å“ªç§ç±»åž‹çš„è§£æžæœ€é€‚åˆæˆ‘ä»¬çš„éœ€æ±‚ã€‚

#### Question 2: Resolving Ambiguity 

**For both constituency and dependency parsing, there can be ambiguity in that there are multiple valid parses for a given sentences. Describe how you could automatically resolve such ambiguities.** 

Answer: 

We could estimate probabilities of parses using a training dataset (Probabilistic parsing). 

We could train a supervised machine learning model to predict which parse is most likely. 

ä½¿ç”¨è®­ç»ƒé›†è¯„ä¼°æ¦‚çŽ‡ï¼Œä¹Ÿå¯ä»¥è®­ç»ƒä¸€ä¸ªç›‘ç£å­¦ä¹ æ¨¡åž‹é¢„æµ‹å“ªä¸ªå¥½ã€‚

#### Question 3: Grammar Practice 

**Given the following grammar ðº=({ð´,ðµ},{ð‘Ž,ð‘},{(ð´â†’ð‘Žðµ),(ð´â†’ðµð‘Ž),(ð´â†’ð‘Žð‘Ž),(ðµâ†’ð‘),(ðµâ†’ð´ð‘)},ð´} ,** 

**a) Is G a context free grammar?** 

Answer: Yes, all production rules have a single non-terminal on the left. 

```
æ˜¯çš„ï¼Œæ‰€æœ‰ç”Ÿäº§è§„åˆ™çš„å·¦ä¾§éƒ½æœ‰ä¸€ä¸ªéžç»ˆç»“ç¬¦
```

**b) Find a valid derivation for the string aaabab** 

Answer: A -> aB -> aAb -> aBab -> aAbab -> aaabab 

**c) Is this grammar in Chomsky normal form?** 

Answer: No, there are rules with one terminal and rules with one non-terminal symbol on the right-hand side. 

```
ä¸ï¼Œåœ¨å³ä¾§æœ‰ä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªç»ˆæ­¢ç¬¦çš„è§„åˆ™å’Œä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªéžç»ˆæ­¢ç¬¦çš„è§„åˆ™ã€‚
ä¹”å§†æ–¯åŸºèŒƒå¼çš„äº§ç”Ÿå¼è¦ä¹ˆå³è¾¹æ˜¯ä¸€ä¸ªç»ˆç»“å­—ç¬¦(å•è¯)ï¼Œè¦ä¹ˆæ˜¯ä¸¤ä¸ªéžç»ˆç»“ç¬¦
```

#### Question 4: Markov Assumption 

**a) Describe the Markov assumption used in language models.** 

Answer: The nth-order Markov assumption states that the probability of a word occurring only depends on what the previous n words were. 

né˜¶é©¬å°”å¯å¤«å‡è®¾æŒ‡å‡ºï¼Œå•è¯å‡ºçŽ°çš„æ¦‚çŽ‡ä»…å–å†³äºŽå‰nä¸ªå•è¯æ˜¯ä»€ä¹ˆã€‚

**b) Write down the chain rule decomposition of the probability ð‘(ð‘¤1,ð‘¤2,ð‘¤3,ð‘¤4), given a second order Markov assumption.** 

Answer: ð‘(ð‘¤1,ð‘¤2,ð‘¤3,ð‘¤4)=ð‘(ð‘¤1)âˆ—ð‘(ð‘¤2|ð‘¤1)âˆ—ð‘(ð‘¤3|ð‘¤1,ð‘¤2)âˆ—ð‘(ð‘¤4|ð‘¤2,ð‘¤3) 

#### Question 5: Interpolation Methods 

**a) Describe what interpolation between language models is. Briefly discuss why this might be useful.** 

Answer: Interpolation refers to estimating probabilities as a weighted sum of higher-order and lower-order language models. This is useful because higher-order models are able to capture longer-term dependency between words however they are prone to overfitting, while lower-order models give reliable estimates of rare words, but do not take into account order. By interpolating, a model can be made which has the strengths of both. 

**b) In general, describe how the use of p_continuation in Kneser-Ney smoothing affects its probability estimates compared to absolute discounting.** 

Answer: p_continuation calculates how likely the word is to appear as a continuation of the previous word, which is not dependent on the number of times the word appears, but rather the number of times it appears as a continuation. This means that if one word is frequent but only ever appears next to a small number of other words (e.g. San Francisco) Kneser-Ney will assign it a lower probability than absolute discounting. 

#### Question 6: Language Model Practice 

**Given the following corpus of text consisting of 3 sentences:** 

**Can the cat catch** 

**Can catch a cat** 

**The cat can catch a can** 

**a) Compute the probability P(cat|can) using absolute discounting.** 

![image-20201112111814130](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112111814130.png)

**b) computer the probability P(cat|can) using Kneser-Ney smoothing.**

![image-20201112111845521](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112111845521.png)

**c) Compute the probability P(cat|can) using stupid back-off.** 

Answer: 0.4 * 3/14 = 0.0857 

**d) Which one of these methods do you think gives a more accurate estimation of the probability p(cat|can)?** 

Answer: For this example, Kneser-Ney would likely give a better estimate since the word cat only appears after 2 unique words. 

**e) Compute the perplexity of the sequence *cat, catch, can* using a stupid back-off model** 

![image-20201112111914795](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112111914795.png)

## 11. Dependency parsing and NLP structures

####  Question 1: Well-formedness 

**For each of the following dependency graphs, state whether they are well-formed, and if not why not.** 

![image-20201112111941015](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201112111941015.png) 

Answer: Only b) is well-formed. a) is non-projective, c) is not connected, and d) has a word with multiple heads. 

#### Question 2: Transition requirements 

**For each of the transitions in Nivreâ€™s arc-eager parsing algorithm, explain what the pre-condition for this transition is and why it is necessary (what would go wrong if the transition was applied when the pre-condition is False?)** 

Answer: 

Left-arc: There needs to be a rule which says that the word at the front of the input can be the head of the word on the top of the stack, and the word on the top of the stack canâ€™t already have a head. This pre-condition needs to be true or else the transition would make the graph multi-headed. 

Right-arc: Same as left-arc, but reversed. 

Reduce: The word on the top of the stack must already have a head. If this condition is false, then applying reduce may make the graph not connected. 

#### Question 3: Nivreâ€™s arc-eager parser 

**Consider the following sentence:** 

**the cat jumped on the fence** 

**[DET, NOUN, VERB, PREP, DET, NOUN]** 

**Run Nivreâ€™s arc-eager parsing algorithm on this sentence with the following set of rules:** 

**Root â†’ Verb,** 

**Verb â†’ Noun,** 

**Noun â†’ Det,** 

**Det â†’ Prep** 

**And this prioritization of transitions:** 

**LA > RA > R > S** 

Answer: 

[ROOT], the cat jumped on the fence, {} 

[the, ROOT], cat jumped on the fence, {} (S) 

[ROOT], cat jumped on the fence, {cat â†’ the} (LA) 

[cat, ROOT], jumped on the fence, {cat â†’ the} (S) 

[ROOT], jumped on the fence, {cat â†’ the, jumped â†’ cat} (LA) 

[jumped, ROOT], on the fence, {cat â†’ the, jumped â†’ cat, ROOT â†’ jumped} (RA) 

[ROOT], on the fence, {cat â†’ the, jumped â†’ cat, ROOT â†’ jumped} (R) 

[on, ROOT], the fence, {cat â†’ the, jumped â†’ cat, ROOT â†’ jumped} (S) 

[ROOT], the fence, {cat â†’ the, jumped â†’ cat, ROOT â†’ jumped, the â†’ on} (LA) 

[the, ROOT], fence, {cat â†’ the, jumped â†’ cat, ROOT â†’ jumped, the â†’ on} (s) 

[ROOT], fence, {cat â†’ the, jumped â†’ cat, ROOT â†’ jumped, the â†’ on, fence â†’ the} (LA) 

[fence, ROOT], ,{cat â†’ the, jumped â†’ cat, ROOT â†’ jumped, the â†’ on, fence â†’ the} (s) 

PARSING FAIL 

#### Question 4: Task structures 

**For each of the following tasks, which data structures would you use to represent the inputs and outputs?** 

**a) Text classification.** 

Answer: Sequence input, vector output. 

**b) Natural language translation.** 

Answer: Sequence input, sequence output. 

**c) Automatic code generation.** 

Answer: Sequence input, tree output. 

**d) Tweet classification.** 

Answer: Graph (of sequences) input, vector output. 

**e) Question-Answering (Given a paragraph of a text, and a natural language question asking something about this text, output the answer to that question).** 

Answer: Graph two sequences as input, sequence output. 

#### Question 5: Task Pipeline 

**For the following tasks, create rough pipelines for solving them. Mention what kind of data you would need to solve the task and which specific architecture you would use (there can be more than one).** 

**A) Convert instructions in plain English about a website UI into HTML + CSS code** 

![image-20201111221032845](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201111221032845.png)

**B) Construct a generative chatbot that can hold a conversation (it shouldnâ€™t just reply to the current input, but hold some context from previous input and output too)** 

![image-20201111221013092](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201111221013092.png)

**C) Predict future stock market prices, using information sources such as news articles and social media.** 

![image-20201111221022943](/Users/lizhuoran/Library/Application Support/typora-user-images/image-20201111221022943.png)